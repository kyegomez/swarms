# `ModelParallelizer` Documentation

## Table of Contents
1. [Understanding the Purpose](#understanding-the-purpose)
2. [Overview and Introduction](#overview-and-introduction)
3. [Class Definition](#class-definition)
4. [Functionality and Usage](#functionality-and-usage)
5. [Additional Information](#additional-information)
6. [Examples](#examples)
7. [Conclusion](#conclusion)

## 1. Understanding the Purpose <a name="understanding-the-purpose"></a>

To create comprehensive documentation for the `ModelParallelizer` class, let's begin by understanding its purpose and functionality.

### Purpose and Functionality

`ModelParallelizer` is a class designed to facilitate the orchestration of multiple Language Model Models (LLMs) to perform various tasks simultaneously. It serves as a powerful tool for managing, distributing, and collecting responses from these models.

Key features and functionality include:

- **Parallel Task Execution**: `ModelParallelizer` can distribute tasks to multiple LLMs and execute them in parallel, improving efficiency and reducing response time.

- **Structured Response Presentation**: The class presents the responses from LLMs in a structured tabular format, making it easy for users to compare and analyze the results.

- **Task History Tracking**: `ModelParallelizer` keeps a record of tasks that have been submitted, allowing users to review previous tasks and responses.

- **Asynchronous Execution**: The class provides options for asynchronous task execution, which can be particularly useful for handling a large number of tasks.

Now that we have an understanding of its purpose, let's proceed to provide a detailed overview and introduction.

## 2. Overview and Introduction <a name="overview-and-introduction"></a>

### Overview

The `ModelParallelizer` class is a crucial component for managing and utilizing multiple LLMs in various natural language processing (NLP) tasks. Its architecture and functionality are designed to address the need for parallel processing and efficient response handling.

### Importance and Relevance

In the rapidly evolving field of NLP, it has become common to use multiple language models to achieve better results in tasks such as translation, summarization, and question answering. `ModelParallelizer` streamlines this process by allowing users to harness the capabilities of several LLMs simultaneously.

Key points:

- **Parallel Processing**: `ModelParallelizer` leverages multithreading to execute tasks concurrently, significantly reducing the time required for processing.

- **Response Visualization**: The class presents responses in a structured tabular format, enabling users to visualize and analyze the outputs from different LLMs.

- **Task Tracking**: Developers can track the history of tasks submitted to `ModelParallelizer`, making it easier to manage and monitor ongoing work.

### Architecture and How It Works

The architecture and working of `ModelParallelizer` can be summarized in four steps:

1. **Task Reception**: `ModelParallelizer` receives a task from the user.

2. **Task Distribution**: The class distributes the task to all registered LLMs.

3. **Response Collection**: `ModelParallelizer` collects the responses generated by the LLMs.

4. **Response Presentation**: Finally, the class presents the responses from all LLMs in a structured tabular format, making it easy for users to compare and analyze the results.

Now that we have an overview, let's proceed with a detailed class definition.

## 3. Class Definition <a name="class-definition"></a>

### Class Attributes

- `llms`: A list of LLMs (Language Model Models) that `ModelParallelizer` manages.

- `last_responses`: Stores the responses from the most recent task.

- `task_history`: Keeps a record of all tasks submitted to `ModelParallelizer`.

### Methods

The `ModelParallelizer` class defines various methods to facilitate task distribution, execution, and response presentation. Let's examine some of the key methods:

- `run(task)`: Distributes a task to all LLMs, collects responses, and returns them.

- `print_responses(task)`: Prints responses from all LLMs in a structured tabular format.

- `run_all(task)`: Runs the task on all LLMs sequentially and returns responses.

- `arun_all(task)`: Asynchronously runs the task on all LLMs and returns responses.

- `print_arun_all(task)`: Prints responses from all LLMs after asynchronous execution.

- `save_responses_to_file(filename)`: Saves responses to a file for future reference.

- `load_llms_from_file(filename)`: Loads LLMs from a file, making it easy to configure `ModelParallelizer` for different tasks.

- `get_task_history()`: Retrieves the task history, allowing users to review previous tasks.

- `summary()`: Provides a summary of task history and the last responses, aiding in post-processing and analysis.

Now that we have covered the class definition, let's delve into the functionality and usage of `ModelParallelizer`.

## 4. Functionality and Usage <a name="functionality-and-usage"></a>

### Distributing a Task and Collecting Responses

One of the primary use cases of `ModelParallelizer` is to distribute a task to all registered LLMs and collect their responses. This can be achieved using the `run(task)` method. Below is an example:

```python
god_mode = ModelParallelizer(llms)
responses = god_mode.run("Translate the following English text to French: 'Hello, how are you?'")
```

### Printing Responses

To present the responses from all LLMs in a structured tabular format, use the `print_responses(task)` method. Example:

```python
god_mode.print_responses("Summarize the main points of 'War and Peace.'")
```

### Saving Responses to a File

Users can save the responses to a file using the `save_responses_to_file(filename)` method. This is useful for archiving and reviewing responses later. Example:

```python
god_mode.save_responses_to_file("responses.txt")
```

### Task History

The `ModelParallelizer` class keeps track of the task history. Developers can access the task history using the `get_task_history()` method. Example:

```python
task_history = god_mode.get_task_history()
for i, task in enumerate(task_history):
    print(f"Task {i + 1}: {task}")
```

## 5. Additional Information <a name="additional-information"></a>

### Parallel Execution

`ModelParallelizer` employs multithreading to execute tasks concurrently. This parallel processing capability significantly improves the efficiency of handling multiple tasks simultaneously.

### Response Visualization

The structured tabular format used for presenting responses simplifies the comparison and analysis of outputs from different LLMs.

## 6. Examples <a name="examples"></a>

Let's explore additional usage examples to illustrate the versatility of `ModelParallelizer` in handling various NLP tasks.

### Example 1: Sentiment Analysis

```python
from swarms.models import OpenAIChat
from swarms.swarms import ModelParallelizer
from swarms.workers.worker import Worker

# Create an instance of an LLM for sentiment analysis
llm = OpenAIChat(model_name="gpt-4", openai_api_key="api-key", temperature=0.5)

# Create worker agents
worker1 = Worker(
    llm=llm,
    ai_name="Bumble Bee",
    ai_role="Worker in a swarm",
    external_tools=None,
    human_in_the_loop=False,
    temperature=0.5,
)
worker2 = Worker

(
    llm=llm,
    ai_name="Optimus Prime",
    ai_role="Worker in a swarm",
    external_tools=None,
    human_in_the_loop=False,
    temperature=0.5,
)
worker3 = Worker(
    llm=llm,
    ai_name="Megatron",
    ai_role="Worker in a swarm",
    external_tools=None,
    human_in_the_loop=False,
    temperature=0.5,
)

# Register the worker agents with ModelParallelizer
agents = [worker1, worker2, worker3]
god_mode = ModelParallelizer(agents)

# Task for sentiment analysis
task = "Please analyze the sentiment of the following sentence: 'This movie is amazing!'"

# Print responses from all agents
god_mode.print_responses(task)
```

### Example 2: Translation

```python
from swarms.models import OpenAIChat

from swarms.swarms import ModelParallelizer

# Define LLMs for translation tasks
translator1 = OpenAIChat(model_name="translator-en-fr", openai_api_key="api-key", temperature=0.7)
translator2 = OpenAIChat(model_name="translator-en-es", openai_api_key="api-key", temperature=0.7)
translator3 = OpenAIChat(model_name="translator-en-de", openai_api_key="api-key", temperature=0.7)

# Register translation agents with ModelParallelizer
translators = [translator1, translator2, translator3]
god_mode = ModelParallelizer(translators)

# Task for translation
task = "Translate the following English text to French: 'Hello, how are you?'"

# Print translated responses from all agents
god_mode.print_responses(task)
```

### Example 3: Summarization

```python
from swarms.models import OpenAIChat

from swarms.swarms import ModelParallelizer


# Define LLMs for summarization tasks
summarizer1 = OpenAIChat(model_name="summarizer-en", openai_api_key="api-key", temperature=0.6)
summarizer2 = OpenAIChat(model_name="summarizer-en", openai_api_key="api-key", temperature=0.6)
summarizer3 = OpenAIChat(model_name="summarizer-en", openai_api_key="api-key", temperature=0.6)

# Register summarization agents with ModelParallelizer
summarizers = [summarizer1, summarizer2, summarizer3]
god_mode = ModelParallelizer(summarizers)

# Task for summarization
task = "Summarize the main points of the article titled 'Climate Change and Its Impact on the Environment.'"

# Print summarized responses from all agents
god_mode.print_responses(task)
```

## 7. Conclusion <a name="conclusion"></a>

In conclusion, the `ModelParallelizer` class is a powerful tool for managing and orchestrating multiple Language Model Models in natural language processing tasks. Its ability to distribute tasks, collect responses, and present them in a structured format makes it invaluable for streamlining NLP workflows. By following the provided documentation, users can harness the full potential of `ModelParallelizer` to enhance their natural language processing projects.

For further information on specific LLMs or advanced usage, refer to the documentation of the respective models and their APIs. Additionally, external resources on parallel execution and response visualization can provide deeper insights into these topics.